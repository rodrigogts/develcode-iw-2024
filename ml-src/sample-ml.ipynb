{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação de Pacotes com `pip`\n",
        "\n",
        "Este comando instala vários pacotes Python necessários para desenvolver modelos de Machine Learning e manipular dados. Vamos detalhar cada parte:\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azureml-sdk pandas scikit-learn matplotlib"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: azureml-sdk in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.57.0)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.3.5)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.5.1)\nRequirement already satisfied: matplotlib in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.6.3)\nRequirement already satisfied: azureml-train-automl-client~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-train-core~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-pipeline~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-dataset-runtime[fuse]~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-core~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-sdk) (1.57.0)\nRequirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas) (2022.5)\nRequirement already satisfied: numpy>=1.21.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib) (9.2.0)\nRequirement already satisfied: azure-mgmt-authorization<5,>=0.40.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (4.0.0)\nRequirement already satisfied: msal<2.0.0,>=1.15.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.30.0)\nRequirement already satisfied: ndg-httpsclient<=0.5.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.5.1)\nRequirement already satisfied: argcomplete<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (3.3.0)\nRequirement already satisfied: paramiko<4.0.0,>=2.0.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (3.4.0)\nRequirement already satisfied: azure-mgmt-storage<=22.0.0,>=16.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (21.2.0)\nRequirement already satisfied: PyJWT<3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (2.4.0)\nRequirement already satisfied: msrest<=0.7.1,>=0.5.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.7.1)\nRequirement already satisfied: backports.tempfile in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.0)\nRequirement already satisfied: contextlib2<22.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (21.6.0)\nRequirement already satisfied: adal<=1.2.7,>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.2.7)\nRequirement already satisfied: knack<0.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.11.0)\nRequirement already satisfied: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.1.28)\nRequirement already satisfied: azure-mgmt-network<=26.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (25.1.0)\nRequirement already satisfied: azure-mgmt-containerregistry<11,>=8.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (10.3.0)\nRequirement already satisfied: msrestazure<=0.7,>=0.4.33 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.6.4.post1)\nRequirement already satisfied: SecretStorage<4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (3.3.3)\nRequirement already satisfied: pathspec<1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.12.1)\nRequirement already satisfied: azure-mgmt-keyvault<11.0.0,>=0.40.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (10.3.0)\nRequirement already satisfied: urllib3<3.0.0,>1.26.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.26.19)\nRequirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.60.0)\nRequirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (2.32.3)\nRequirement already satisfied: humanfriendly<11.0,>=4.7 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (10.0)\nRequirement already satisfied: azure-mgmt-resource<=24.0.0,>=15.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (23.1.1)\nRequirement already satisfied: docker<8.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (7.1.0)\nRequirement already satisfied: pyopenssl<25.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (23.0.0)\nRequirement already satisfied: jmespath<2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (0.10.0)\nRequirement already satisfied: jsonpickle<4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (3.2.2)\nRequirement already satisfied: msal-extensions<=2.0.0,>=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.2.0)\nRequirement already satisfied: pkginfo in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.11.1)\nRequirement already satisfied: azure-core<2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-core~=1.57.0->azureml-sdk) (1.30.2)\nRequirement already satisfied: pyarrow>=0.17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (14.0.2)\nRequirement already satisfied: azureml-dataprep<5.2.0a,>=5.1.0a in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (5.1.6)\nRequirement already satisfied: fusepy<4.0.0,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (3.0.1)\nRequirement already satisfied: azureml-pipeline-steps~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-pipeline~=1.57.0->azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-pipeline-core~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-pipeline~=1.57.0->azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-automl-core~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-train-automl-client~=1.57.0->azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-telemetry~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-train-automl-client~=1.57.0->azureml-sdk) (1.57.0)\nRequirement already satisfied: azureml-train-restclients-hyperdrive~=1.57.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-train-core~=1.57.0->azureml-sdk) (1.57.0)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\nRequirement already satisfied: cryptography>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from adal<=1.2.7,>=1.2.0->azureml-core~=1.57.0->azureml-sdk) (38.0.4)\nRequirement already satisfied: typing-extensions>=4.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core<2.0.0->azureml-core~=1.57.0->azureml-sdk) (4.12.2)\nRequirement already satisfied: isodate<1.0.0,>=0.6.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-mgmt-authorization<5,>=0.40.0->azureml-core~=1.57.0->azureml-sdk) (0.6.1)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-mgmt-authorization<5,>=0.40.0->azureml-core~=1.57.0->azureml-sdk) (1.4.0)\nRequirement already satisfied: importlib-metadata<=8.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-automl-core~=1.57.0->azureml-train-automl-client~=1.57.0->azureml-sdk) (8.2.0)\nRequirement already satisfied: importlib-resources<=6.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-automl-core~=1.57.0->azureml-train-automl-client~=1.57.0->azureml-sdk) (6.4.0)\nRequirement already satisfied: azureml-dataprep-rslex~=2.22.2dev0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (2.22.2)\nRequirement already satisfied: azure-identity>=1.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (1.17.1)\nRequirement already satisfied: azureml-dataprep-native<42.0.0,>=41.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (41.0.0)\nRequirement already satisfied: cloudpickle<3.0.0,>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (2.2.1)\nRequirement already satisfied: jsonschema in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (4.23.0)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (6.0.1)\nRequirement already satisfied: applicationinsights in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azureml-telemetry~=1.57.0->azureml-train-automl-client~=1.57.0->azureml-sdk) (0.11.10)\nRequirement already satisfied: pygments in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from knack<0.12.0->azureml-core~=1.57.0->azureml-sdk) (2.18.0)\nRequirement already satisfied: tabulate in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from knack<0.12.0->azureml-core~=1.57.0->azureml-sdk) (0.9.0)\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msal-extensions<=2.0.0,>=0.3.0->azureml-core~=1.57.0->azureml-sdk) (2.10.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.57.0->azureml-sdk) (2024.8.30)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.57.0->azureml-sdk) (2.0.0)\nRequirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.57.0->azureml-sdk) (0.6.0)\nRequirement already satisfied: pynacl>=1.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.57.0->azureml-sdk) (1.5.0)\nRequirement already satisfied: bcrypt>=3.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from paramiko<4.0.0,>=2.0.8->azureml-core~=1.57.0->azureml-sdk) (4.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.57.0->azureml-sdk) (3.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.57.0->azureml-sdk) (3.3.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.57.0->azureml-sdk) (1.7.1)\nRequirement already satisfied: jeepney>=0.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from SecretStorage<4.0.0->azureml-core~=1.57.0->azureml-sdk) (0.8.0)\nRequirement already satisfied: backports.weakref in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from backports.tempfile->azureml-core~=1.57.0->azureml-sdk) (1.0.post1)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core~=1.57.0->azureml-sdk) (1.16.0)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib-metadata<=8.2.0->azureml-automl-core~=1.57.0->azureml-train-automl-client~=1.57.0->azureml-sdk) (3.20.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core~=1.57.0->azureml-sdk) (3.2.2)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (0.20.0)\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (24.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<5.2.0a,>=5.1.0a->azureml-dataset-runtime[fuse]~=1.57.0->azureml-sdk) (2023.12.1)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core~=1.57.0->azureml-sdk) (2.22)\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecção de Anomalias com Isolation Forest\n\nEste código realiza detecção de anomalias em um conjunto de dados numéricos utilizando o algoritmo **Isolation Forest**. Ele inclui as etapas de carregamento, pré-processamento e treinamento do modelo, além de salvar o modelo treinado para uso futuro. Abaixo, cada etapa é explicada em detalhes:\n\n## Bibliotecas Utilizadas\n\n- **pandas**: Utilizado para carregar e manipular o conjunto de dados.\n- **sklearn.preprocessing.StandardScaler**: Utilizado para escalar os dados numéricos, garantindo que todas as features tenham a mesma escala.\n- **sklearn.ensemble.IsolationForest**: Algoritmo de detecção de anomalias utilizado para identificar dados fora do padrão.\n- **joblib**: Biblioteca usada para salvar o modelo treinado em um arquivo, permitindo reutilizá-lo sem precisar treiná-lo novamente.\n\n## Etapa 1: Carregamento dos Dados\n\n```python\nfile_path = 'order_status_code.csv'\ndata = pd.read_csv(file_path)\n```\n\n- O código carrega um arquivo CSV chamado `order_status_code.csv` contendo os dados que serão usados para detecção de anomalias.\n\n## Etapa 2: Pré-processamento\n\n- Primeiro, o código verifica os tipos de dados das colunas para identificar se há colunas de tipo `datetime`.\n\n```python\nprint(\"Data types:\")\nprint(data.dtypes)\n```\n\n- Se houver uma coluna de data/hora chamada `datetime_column`, ela é convertida para o formato `datetime`. Em seguida, recursos úteis, como **hora**, **dia**, **mês** e **ano**, são extraídos e a coluna original é removida.\n\n```python\nif 'datetime_column' in data.columns:\n    data['datetime_column'] = pd.to_datetime(data['datetime_column'])\n    data['hour'] = data['datetime_column'].dt.hour\n    data['day'] = data['datetime_column'].dt.day\n    data['month'] = data['datetime_column'].dt.month\n    data['year'] = data['datetime_column'].dt.year\n    data = data.drop(columns=['datetime_column'])\n```\n\n## Etapa 3: Seleção de Colunas Numéricas\n\n```python\ndata_numeric = data.select_dtypes(include=[float, int])\ndata_numeric = data_numeric.dropna().drop_duplicates()\n```\n\n- Apenas colunas numéricas são selecionadas, já que o algoritmo **Isolation Forest** trabalha apenas com dados numéricos.\n- Linhas com valores ausentes ou duplicados são removidas para garantir a qualidade dos dados.\n\n## Etapa 4: Escalonamento dos Dados\n\n```python\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data_numeric)\n```\n\n- Os dados numéricos são escalonados usando `StandardScaler` para padronizar as variáveis, ou seja, ajustá-las para terem média zero e desvio padrão 1. Isso melhora o desempenho do modelo.\n\n## Etapa 5: Treinamento do Modelo de Detecção de Anomalias\n\n```python\nmodel = IsolationForest(contamination=0.01)\nmodel.fit(X_scaled)\n```\n\n- O modelo **Isolation Forest** é instanciado e treinado usando os dados escalados.\n- O parâmetro `contamination=0.01` indica que o modelo deve considerar 1% dos dados como anômalos.\n\n## Etapa 6: Salvamento do Modelo Treinado\n\n```python\njoblib.dump(model, 'anomaly_detection_model.pkl')\n```\n\n- O modelo treinado é salvo em um arquivo `.pkl` usando a biblioteca `joblib`. Isso permite que o modelo seja carregado e utilizado posteriormente sem precisar ser treinado novamente.\n\n## Conclusão\n\nEste código realiza a detecção de anomalias em um conjunto de dados numéricos usando o **Isolation Forest**. Ele inclui todas as etapas essenciais para carregar os dados, pré-processá-los, treinar o modelo e, finalmente, salvar o modelo para uso futuro. O modelo pode ser usado em um ambiente de produção para detectar anomalias em novos dados.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load your data\n",
        "file_path = 'order_status_code.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Preprocessing (Handle datetime columns, select only numerical columns, etc.)\n",
        "# Check if there are any datetime columns that need to be converted\n",
        "print(\"Data types:\")\n",
        "print(data.dtypes)\n",
        "\n",
        "# If you have a datetime column, convert it and extract useful features (like hour, day, etc.)\n",
        "# Example: Assuming there's a column named 'datetime_column'\n",
        "if 'datetime_column' in data.columns:\n",
        "    data['datetime_column'] = pd.to_datetime(data['datetime_column'])\n",
        "    data['hour'] = data['datetime_column'].dt.hour\n",
        "    data['day'] = data['datetime_column'].dt.day\n",
        "    data['month'] = data['datetime_column'].dt.month\n",
        "    data['year'] = data['datetime_column'].dt.year\n",
        "    # Drop the original datetime column\n",
        "    data = data.drop(columns=['datetime_column'])\n",
        "\n",
        "# Step 3: Select only numeric columns for scaling and training\n",
        "data_numeric = data.select_dtypes(include=[float, int])\n",
        "\n",
        "# Remove rows with missing values and duplicates\n",
        "data_numeric = data_numeric.dropna().drop_duplicates()\n",
        "\n",
        "# Step 4: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data_numeric)\n",
        "\n",
        "# Step 5: Train the Isolation Forest model for anomaly detection\n",
        "# Adjust the contamination as needed (1% anomalies)\n",
        "model = IsolationForest(contamination=0.01) \n",
        "model.fit(X_scaled)\n",
        "\n",
        "# Step 6: Save the trained model for deployment\n",
        "joblib.dump(model, 'anomaly_detection_model.pkl')\n",
        "\n",
        "print(\"Model training complete and saved as 'anomaly_detection_model.pkl'\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data types:\ncode    int64\ndtype: object\nModel training complete and saved as 'anomaly_detection_model.pkl'\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728950452152
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conectando ao Workspace do Azure Machine Learning\n\nEste código configura o ambiente do Azure Machine Learning e estabelece uma conexão com o workspace. Aqui está uma explicação detalhada de cada parte:\n\n## 1. **Importando a Classe Workspace**\n\n```python\nfrom azureml.core import Workspace\n```\n\n- O **Workspace** é um contêiner central no Azure Machine Learning que contém todos os seus ativos de machine learning, como experimentos, modelos, pipelines e computação. \n- A função `Workspace` vem do SDK do **AzureML** e é utilizada para conectar-se ao ambiente de trabalho da sua conta no Azure.\n\n## 2. **Conectando ao Workspace do Azure ML**\n\n```python\nws = Workspace.from_config()\n```\n\n- Este comando tenta carregar a configuração do **Workspace** a partir de um arquivo `config.json`, que contém os detalhes de autenticação e identificação do seu workspace (como o nome do recurso, assinatura do Azure e grupo de recursos).\n- Se o arquivo `config.json` estiver corretamente configurado no diretório do projeto, essa linha estabelecerá a conexão automaticamente.\n- Caso contrário, seria necessário passar as informações manualmente (por exemplo, nome do workspace, ID de assinatura do Azure e nome do grupo de recursos).\n\n## 3. **Exibindo Detalhes do Workspace**\n\n```python\nws.get_details()\n```\n\n- Esta linha exibe informações sobre o workspace ao qual você está conectado, como:\n  - Nome do workspace\n  - Localização\n  - ID de assinatura do Azure\n  - Grupo de recursos associado\n  - Outras propriedades do workspace\n\n## Resumo\n\nEste código estabelece uma conexão com o **Azure Machine Learning Workspace** usando o SDK do Azure ML. Ele facilita o gerenciamento de recursos e experimentos em um ambiente de machine learning na nuvem. A função `from_config()` simplifica o processo, permitindo que as credenciais sejam carregadas automaticamente de um arquivo de configuração. Uma vez conectado, você pode começar a realizar operações de machine learning, como criação de experimentos, gerenciamento de modelos e implantação."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the Azure environment\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Connect to Azure ML workspace (use config.json or manually input workspace details)\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Display workspace details\n",
        "ws.get_details()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "{'id': '/subscriptions/32855dba-b7b2-4fdb-8288-f0615afa0cdf/resourceGroups/rg-evaluations/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-training',\n 'name': 'ml-workspace-training',\n 'identity': {'principal_id': '739ef407-6415-40b3-8bc1-6ba9fafc63bf',\n  'tenant_id': '6bd72601-6bad-49d9-82df-a390b167cd68',\n  'type': 'SystemAssigned'},\n 'location': 'swedencentral',\n 'type': 'Microsoft.MachineLearningServices/workspaces',\n 'tags': {},\n 'sku': 'Basic',\n 'workspaceid': '2e136fb4-42a8-4e2f-8d62-51567db61c8f',\n 'sdkTelemetryAppInsightsKey': 'ecfc0110-6630-4d5b-935f-f8d304673557',\n 'description': '',\n 'friendlyName': 'Machine Learning Training Workspace',\n 'creationTime': '2024-10-14T17:02:16.9671419Z',\n 'containerRegistry': '/subscriptions/32855dba-b7b2-4fdb-8288-f0615afa0cdf/resourceGroups/rg-evaluations/providers/Microsoft.ContainerRegistry/registries/2e136fb442a84e2f8d6251567db61c8f',\n 'keyVault': '/subscriptions/32855dba-b7b2-4fdb-8288-f0615afa0cdf/resourceGroups/rg-evaluations/providers/Microsoft.Keyvault/vaults/mlworkspacetra8219805002',\n 'applicationInsights': '/subscriptions/32855dba-b7b2-4fdb-8288-f0615afa0cdf/resourceGroups/rg-evaluations/providers/Microsoft.insights/components/mlworkspacetra4958510099',\n 'storageAccount': '/subscriptions/32855dba-b7b2-4fdb-8288-f0615afa0cdf/resourceGroups/rg-evaluations/providers/Microsoft.Storage/storageAccounts/mlworkspacetra9273830648',\n 'hbiWorkspace': False,\n 'provisioningState': 'Succeeded',\n 'discoveryUrl': 'https://swedencentral.api.azureml.ms/discovery',\n 'notebookInfo': {'fqdn': 'ml-ml-worksp-swedencentral-2e136fb4-42a8-4e2f-8d62-51567db61c8f.swedencentral.notebooks.azure.net',\n  'resource_id': '4c3c5856688f4abf8b4ac434fb79cff3'},\n 'v1LegacyMode': False}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728950663852
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Registrando um Modelo Treinado no Azure Machine Learning\n\nEste código faz o registro de um modelo de machine learning no **Azure Machine Learning Workspace**. A seguir, está a explicação de cada parte:\n\n## 1. **Importando a Classe `Model`**\n\n```python\nfrom azureml.core.model import Model\n```\n\n- A classe `Model` do **AzureML SDK** é usada para gerenciar modelos no workspace. Ela permite registrar, listar e baixar modelos para experimentos e deployment.\n- O registro de um modelo no Azure facilita a rastreabilidade e reutilização do modelo para futuras implementações ou avaliações.\n\n## 2. **Registrando o Modelo no Workspace**\n\n```python\nmodel = Model.register(\n    workspace=ws, \n    model_path=\"anomaly_detection_model.pkl\",  # Caminho para o modelo treinado\n    model_name=\"anomaly_detection_model\"  # Nome do modelo registrado no Azure\n)\n```\n\n- A função `Model.register()` realiza o registro do modelo no **workspace** do Azure.\n- **Parâmetros**:\n  - `workspace`: refere-se ao workspace do Azure Machine Learning em que o modelo será registrado. Esse workspace já foi criado e conectado na etapa anterior (`ws`).\n  - `model_path`: o caminho local do arquivo do modelo treinado (neste caso, `anomaly_detection_model.pkl`), que será carregado e registrado no Azure.\n  - `model_name`: o nome com o qual o modelo será registrado no Azure Machine Learning, neste exemplo, `\"anomaly_detection_model\"`. Esse nome será usado para identificar e acessar o modelo posteriormente.\n\n## 3. **Verificando o Registro do Modelo**\n\n```python\nprint(f\"Model registered with ID: {model.id}\")\n```\n\n- Depois que o modelo é registrado com sucesso, o código exibe o **ID do modelo** no workspace. Esse ID pode ser utilizado para referenciar o modelo em futuras operações, como ao fazer deploy ou monitorar o desempenho do modelo.\n\n## Resumo\n\nEste código registra um modelo treinado no **Azure Machine Learning**. O registro do modelo facilita sua gestão e reutilização, permitindo que seja facilmente implantado em novos ambientes ou compartilhado com outros membros da equipe. A função `Model.register()` carrega o arquivo do modelo do caminho especificado e o associa ao workspace no Azure, atribuindo um nome e gerando um ID único para rastrear o modelo. Esse processo é essencial em um fluxo de machine learning, especialmente em ambientes de produção e colaborativos."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Register the trained model in Azure\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# Register the model in the workspace\n",
        "model = Model.register(workspace=ws,\n",
        "                       model_path=\"anomaly_detection_model.pkl\",  # Path to the trained model\n",
        "                       model_name=\"anomaly_detection_model\")  # Name of the model in Azure\n",
        "\n",
        "# Print the model ID to verify registration\n",
        "print(f\"Model registered with ID: {model.id}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model anomaly_detection_model\nModel registered with ID: anomaly_detection_model:4\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728950682038
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando um Ambiente para Inferência de Modelos no Azure Machine Learning\n\nEste código cria e configura um ambiente personalizado no **Azure Machine Learning** para ser usado durante a inferência de um modelo de machine learning. Abaixo está uma explicação detalhada de cada parte do código.\n\n## 1. **Importando a Classe `Environment`**\n\n```python\nfrom azureml.core import Environment\n```\n\n- A classe `Environment` do **AzureML SDK** é usada para definir o ambiente em que os modelos serão treinados ou executados. Um ambiente contém as bibliotecas e dependências necessárias para que o código seja executado corretamente no Azure, seja em um processo de treinamento ou durante a inferência.\n\n## 2. **Criando o Ambiente para Inferência**\n\n```python\nenv = Environment(name=\"anomaly-env\")\n```\n\n- Aqui, um novo ambiente chamado `\"anomaly-env\"` é criado. Esse ambiente será utilizado para hospedar o modelo de detecção de anomalias e garantir que todas as dependências necessárias estejam presentes para realizar a inferência de dados.\n- Um **ambiente** é uma forma de garantir que o código do modelo tenha todas as bibliotecas e versões corretas disponíveis quando for executado em um contêiner no Azure.\n\n## 3. **Adicionando Dependências ao Ambiente**\n\n```python\nenv.python.conda_dependencies.add_pip_package(\"scikit-learn\")\nenv.python.conda_dependencies.add_pip_package(\"joblib\")\nenv.python.conda_dependencies.add_pip_package(\"numpy\")\nenv.python.conda_dependencies.add_pip_package(\"azureml-inference-server-http\")\n```\n\n- Esta parte do código adiciona as bibliotecas necessárias ao ambiente.\n  - **`scikit-learn`**: É a biblioteca que contém o algoritmo de **Isolation Forest** e outras ferramentas de machine learning.\n  - **`joblib`**: Usada para salvar e carregar o modelo treinado. A `joblib` permite serializar e desserializar o modelo de forma eficiente.\n  - **`numpy`**: Biblioteca fundamental para cálculos numéricos e manipulação de arrays, amplamente usada em machine learning.\n  - **`azureml-inference-server-http`**: Este pacote é essencial para hospedar o modelo no Azure e permitir que ele seja acessado via uma API HTTP para realizar inferências (predições) sobre novos dados.\n\n## Resumo\n\nEste código configura um **ambiente** personalizado no **Azure Machine Learning** para a execução de inferências de um modelo de detecção de anomalias. Ele cria o ambiente chamado `\"anomaly-env\"` e adiciona todas as dependências necessárias (bibliotecas Python) para que o modelo seja executado corretamente no ambiente de produção, permitindo que o modelo faça predições sobre novos dados. O uso de ambientes no Azure ML garante que o código seja executado de forma consistente e previsível em diferentes fases, seja no desenvolvimento, teste ou produção."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "# Criar o ambiente para a inferência do modelo\n",
        "env = Environment(name=\"anomaly-env\")\n",
        "\n",
        "# Adicionar pacotes necessários (como scikit-learn, joblib, numpy e o servidor HTTP)\n",
        "env.python.conda_dependencies.add_pip_package(\"scikit-learn\")\n",
        "env.python.conda_dependencies.add_pip_package(\"joblib\")\n",
        "env.python.conda_dependencies.add_pip_package(\"numpy\")\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-inference-server-http\")\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728933377430
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração de Inferência no Azure Machine Learning\n\nEste código define a configuração necessária para implantar um modelo de machine learning no **Azure Machine Learning** com base no script de inferência e no ambiente criado anteriormente. Abaixo está a explicação detalhada de cada parte:\n\n## 1. **Importando a Classe `InferenceConfig`**\n\n```python\nfrom azureml.core.model import InferenceConfig\n```\n\n- A classe `InferenceConfig` do **AzureML SDK** é usada para configurar como o modelo será implantado e executado para inferência. Ela define o script de entrada e o ambiente onde o modelo será carregado e usado para realizar predições.\n\n## 2. **Definindo a Configuração de Inferência**\n\n```python\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n```\n\n- Aqui, é criada uma instância da classe `InferenceConfig`, que especifica as informações necessárias para a execução do modelo na fase de inferência.\n\n### Parâmetros:\n- **`entry_script=\"score.py\"`**:\n  - O script de entrada que será usado para executar o modelo. O arquivo `score.py` geralmente contém a lógica de carregamento do modelo, o processamento de entrada e a geração de predições.\n  - Dentro do script `score.py`, funções como `init()` (para inicializar o modelo) e `run()` (para realizar inferências sobre os dados de entrada) são definidas.\n  \n- **`environment=env`**:\n  - O ambiente que foi criado anteriormente no código, chamado `\"anomaly-env\"`, contendo todas as dependências necessárias (bibliotecas como `scikit-learn`, `joblib`, `numpy` e o servidor HTTP do Azure).\n  - Isso garante que o modelo terá acesso às mesmas bibliotecas e versões que foram usadas durante o treinamento ou preparação para a inferência.\n\n## Resumo\n\nEste código define a **configuração de inferência** para implantar o modelo no **Azure Machine Learning**. Ele associa o script de inferência, `score.py`, que contém a lógica para carregar e executar o modelo, ao ambiente previamente configurado (`env`). O arquivo `score.py` é fundamental, pois ele determina como o modelo processa dados de entrada e produz predições, enquanto o ambiente garante que todas as dependências necessárias estejam disponíveis durante a execução do modelo.\n\nEssa configuração é crucial para a implantação de modelos no Azure, permitindo que o modelo seja acessado e utilizado por meio de uma API ou interface de inferência para previsões em tempo real."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "# Definir a configuração de inferência usando o arquivo score.py\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728933228486
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulando `init()` e `run()` para Inferência de Anomalias com Azure ML\n\nEste código simula as funções principais que são geralmente usadas em um script de inferência no **Azure Machine Learning**, como o arquivo `score.py`. Ele inclui a inicialização e execução do modelo para realizar predições com base em dados de entrada e é estruturado para detectar anomalias.\n\n## 1. **Importando Bibliotecas Necessárias**\n\n```python\nimport json\nimport numpy as np\nimport joblib\nimport os\nimport random\n```\n\n- **`json`**: Usado para carregar e processar os dados de entrada que estão no formato JSON.\n- **`numpy`**: Usado para manipular os dados de entrada como arrays NumPy, que são a estrutura esperada para a maioria dos algoritmos de machine learning.\n- **`joblib`**: Biblioteca usada para carregar o modelo previamente salvo em um arquivo `.pkl`.\n- **`os`**: Biblioteca para interagir com o sistema de arquivos, como verificar se o modelo existe.\n- **`random`**: Utilizada para gerar dados de teste com uma distribuição específica para simular erros HTTP.\n\n## 2. **Função `init()`**\n\n```python\ndef init():\n    global model\n    model_path = 'anomaly_detection_model.pkl'\n    \n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Modelo não encontrado no caminho: {model_path}\")\n    \n    model = joblib.load(model_path)\n```\n\n- A função `init()` é responsável por inicializar o modelo. Em um ambiente de produção no Azure, o modelo seria carregado do diretório `AZUREML_MODEL_DIR`, mas aqui é usado o caminho local `'anomaly_detection_model.pkl'`.\n- Se o modelo não for encontrado no caminho especificado, é gerado um erro `FileNotFoundError`.\n- O modelo é carregado usando o **`joblib.load()`**, que desserializa o modelo para estar pronto para realizar inferências.\n\n## 3. **Função `run()`**\n\n```python\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['input']\n        input_data = np.array(data)\n        \n        expected_features = model.n_features_in_\n        if input_data.shape[1] != expected_features:\n            raise ValueError(f\"O modelo espera {expected_features} features, mas os dados de entrada possuem {input_data.shape[1]} features.\")\n        \n        predictions = model.predict(input_data)\n        alerts = [f\"Anomaly detected at row {i}\" for i, pred in enumerate(predictions) if pred == -1]\n        \n        return {\"predictions\": predictions.tolist(), \"alerts\": alerts}\n    except Exception as e:\n        return {\"error\": str(e)}\n```\n\n- **Entrada**:\n  - Recebe os dados de entrada em formato JSON. O JSON deve conter uma chave chamada `'input'` que armazena os dados em formato de matriz (ou lista de listas).\n  \n- **Processamento**:\n  - Converte os dados de entrada de JSON para uma matriz NumPy.\n  - Verifica se o número de **features** (colunas) no conjunto de dados de entrada corresponde ao número de features para o qual o modelo foi treinado. Se o número de features não corresponder, um erro é gerado.\n  \n- **Inferência**:\n  - Usa o modelo carregado para fazer previsões. No caso de anomalias, o modelo retorna `-1`, indicando que os dados correspondentes são anômalos.\n  \n- **Alertas**:\n  - Gera uma lista de alertas para cada linha de entrada onde uma anomalia (`-1`) foi detectada.\n\n- **Saída**:\n  - Retorna um JSON contendo as **previsões** e os **alertas** gerados.\n\n## 4. **Execução Local para Testes**\n\n```python\nif __name__ == \"__main__\":\n    init()\n\n    http_statuses = [500] * 85 + random.choices([403, 500, 400], k=15)\n    \n    test_input = json.dumps({\n        \"input\": [[status] for status in http_statuses]\n    })\n    \n    result = run(test_input)\n    print(result)\n```\n\n- **Simulação de Erros HTTP**:\n  - Aqui, são gerados códigos de status HTTP, onde 85% das requisições são 500 (indicado como \"normais\"), e 15% são erros HTTP aleatórios (403, 400 e 500) simulando dados de entrada.\n  \n- **Testes**:\n  - A função `run()` é chamada com esses dados simulados para verificar como o modelo detecta anomalias. O resultado é impresso no console.\n\n## Resumo\n\nEste código simula o processo de inferência para detecção de anomalias com base em um modelo de **Isolation Forest** no Azure Machine Learning. As funções **`init()`** e **`run()`** são típicas de scripts de inferência no Azure, onde o modelo é carregado uma vez e as predições são feitas em dados de entrada recebidos em formato JSON. O código também inclui um teste local para verificar como o modelo lida com dados simulados e detectar anomalias entre os códigos de status HTTP fornecidos."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Simular a função init() do score.py\n",
        "def init():\n",
        "    \"\"\"\n",
        "    Esta função inicializa o modelo. Em produção, o modelo seria carregado do diretório\n",
        "    AZUREML_MODEL_DIR, mas aqui podemos carregar o modelo diretamente do local.\n",
        "    \"\"\"\n",
        "    global model\n",
        "    model_path = 'anomaly_detection_model.pkl'  # Caminho local para testar\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Modelo não encontrado no caminho: {model_path}\")\n",
        "    \n",
        "    # Carregar o modelo com joblib\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "# Simular a função run() do score.py\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    Esta função processa os dados de entrada e realiza a inferência.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Analisar o JSON de entrada\n",
        "        data = json.loads(raw_data)['input']\n",
        "        \n",
        "        # Converter os dados em uma matriz NumPy para realizar a previsão\n",
        "        input_data = np.array(data)\n",
        "        \n",
        "        # Verificar quantas features o modelo foi treinado para receber\n",
        "        expected_features = model.n_features_in_  # Número de features esperado pelo modelo\n",
        "        if input_data.shape[1] != expected_features:\n",
        "            raise ValueError(f\"O modelo espera {expected_features} features, mas os dados de entrada possuem {input_data.shape[1]} features.\")\n",
        "        \n",
        "        # Fazer as previsões usando o modelo carregado\n",
        "        predictions = model.predict(input_data)\n",
        "        \n",
        "        # Gerar alertas para quaisquer anomalias detectadas (previsão = -1)\n",
        "        alerts = [f\"Anomaly detected at row {i}\" for i, pred in enumerate(predictions) if pred == -1]\n",
        "        \n",
        "        # Retornar as previsões e os alertas como um JSON\n",
        "        return {\"predictions\": predictions.tolist(), \"alerts\": alerts}\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Simular a execução local para testar\n",
        "if __name__ == \"__main__\":\n",
        "    init()  # Inicializar o modelo\n",
        "\n",
        "    # Gerar percentual de erro de 15%\n",
        "    http_statuses = [200] * 85 + random.choices([403, 500, 400], k=15)\n",
        "    \n",
        "    # Exemplo de dados de entrada no formato JSON\n",
        "    test_input = json.dumps({\n",
        "        \"input\": [[status] for status in http_statuses] \n",
        "    })\n",
        "    \n",
        "    # Executar a função run() com os dados de teste\n",
        "    result = run(test_input)\n",
        "    print(result)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'predictions': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'alerts': []}\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728950856234
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deletando um Serviço Web no Azure Machine Learning\n\nEste código carrega um serviço web já existente no **Azure Machine Learning** e, em seguida, exclui esse serviço. Abaixo está a explicação de cada parte do código:\n\n## 1. **Importando a Classe `Webservice`**\n\n```python\nfrom azureml.core.webservice import Webservice\n```\n\n- A classe `Webservice` do **AzureML SDK** é usada para gerenciar serviços web que foram implantados para hospedar modelos de machine learning no Azure. Esses serviços permitem que o modelo seja acessado por meio de uma API para realizar inferências.\n\n## 2. **Carregando o Serviço Web Existente**\n\n```python\nservice = Webservice(workspace=ws, name=\"anomaly-detector-service\")\n```\n\n- Esta linha carrega um serviço web que já foi implantado anteriormente. \n- **Parâmetros**:\n  - **`workspace=ws`**: O workspace do Azure Machine Learning onde o serviço está hospedado. Esse workspace deve estar previamente configurado e conectado.\n  - **`name=\"anomaly-detector-service\"`**: O nome do serviço web que você deseja gerenciar ou deletar. Esse nome deve corresponder ao nome do serviço registrado no workspace.\n\n## 3. **Excluindo o Serviço Web**\n\n```python\nservice.delete()\n```\n\n- A função `delete()` é chamada para excluir o serviço web especificado. Isso significa que o modelo não estará mais disponível para ser acessado por uma API, e os recursos associados a ele, como a infraestrutura de contêineres, serão liberados.\n- Esta ação é irreversível: uma vez que o serviço é deletado, ele precisará ser reimplantado caso seja necessário novamente.\n\n## 4. **Confirmando a Exclusão**\n\n```python\nprint(\"Serviço deletado com sucesso.\")\n```\n\n- Após a exclusão bem-sucedida, uma mensagem é exibida no console confirmando que o serviço foi deletado.\n\n## Resumo\n\nEste código carrega um serviço web já implantado no **Azure Machine Learning** e o exclui. O serviço web é responsável por hospedar um modelo de machine learning, permitindo que ele seja acessado via API para inferências. A função `delete()` remove o serviço e libera os recursos associados, interrompendo o acesso ao modelo. A exclusão de um serviço pode ser útil quando você não precisa mais dele, economizando custos e recursos computacionais no Azure."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import Webservice\n",
        "\n",
        "# Carregar o serviço existente\n",
        "service = Webservice(workspace=ws, name=\"anomaly-detector-service\")\n",
        "\n",
        "# Excluir o serviço\n",
        "service.delete()\n",
        "print(\"Serviço deletado com sucesso.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running\n2024-10-14 19:57:10+00:00 Deleting service.\n2024-10-14 19:57:15+00:00 Deleting service entity.\nSucceeded\nServiço deletado com sucesso.\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728935836681
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implantando um Modelo no Azure Container Instances (ACI)\n\nEste código implanta um modelo de machine learning no **Azure Container Instances (ACI)**, um serviço gerenciado do Azure que permite implantar contêineres sem precisar gerenciar a infraestrutura subjacente. O modelo será disponibilizado como um serviço web para realizar inferências em tempo real. Abaixo está a explicação de cada parte do código.\n\n## 1. **Importando Bibliotecas Necessárias**\n\n```python\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import Model\n```\n\n- **`AciWebservice`**: Classe usada para configurar e implantar um serviço de modelo no **Azure Container Instances (ACI)**.\n- **`Model`**: Classe usada para carregar e gerenciar o modelo previamente registrado no workspace do **Azure Machine Learning**.\n\n## 2. **Configurando o Deployment para o ACI**\n\n```python\naci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n```\n\n- Aqui, uma configuração de implantação para o **ACI** é criada, definindo os recursos de computação a serem alocados para o contêiner que hospedará o modelo.\n- **Parâmetros**:\n  - **`cpu_cores=1`**: Define a quantidade de CPUs a ser alocada para o contêiner.\n  - **`memory_gb=1`**: Define a quantidade de memória (em GB) a ser alocada para o contêiner.\n  \nEssas configurações são ajustáveis dependendo da complexidade do modelo e da carga esperada.\n\n## 3. **Implantando o Modelo no ACI**\n\n```python\nservice = Model.deploy(\n    workspace=ws,\n    name=\"anomaly-detector-service\",\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config\n)\n```\n\n- **`Model.deploy()`**: Esta função implanta o modelo no **ACI**. Os parâmetros necessários incluem:\n  - **`workspace=ws`**: O workspace do Azure Machine Learning onde o modelo está registrado.\n  - **`name=\"anomaly-detector-service\"`**: O nome do serviço que será criado e exposto para inferências.\n  - **`models=[model]`**: O modelo que foi registrado no workspace e que será implantado. Este modelo já foi carregado anteriormente.\n  - **`inference_config=inference_config`**: A configuração de inferência, que define o script de pontuação (`score.py`) e o ambiente com as dependências necessárias (configurado anteriormente).\n  - **`deployment_config=aci_config`**: A configuração do ACI, que define os recursos computacionais para o contêiner.\n\n## 4. **Aguardando a Conclusão da Implantação**\n\n```python\nservice.wait_for_deployment(show_output=True)\n```\n\n- Esta função aguarda a conclusão do processo de implantação, exibindo no console as mensagens de log associadas ao processo. Quando a implantação for concluída, o serviço estará disponível para uso.\n\n## 5. **Exibindo a URI de Pontuação**\n\n```python\nprint(f\"Service deployed at: {service.scoring_uri}\")\n```\n\n- Após a implantação ser concluída, o código exibe a **URI de pontuação** (ou scoring URI), que é a URL na qual o serviço pode ser acessado para realizar inferências via API.\n- As requisições de inferência podem ser enviadas para essa URI com dados no formato apropriado (geralmente JSON) para obter as predições do modelo.\n\n## Resumo\n\nEste código faz o deployment de um modelo de machine learning no **Azure Container Instances (ACI)**. O processo inclui a configuração de recursos de computação, a definição de como o modelo será executado para realizar inferências e, finalmente, a implantação do serviço web no ACI. Ao término, o serviço estará acessível por uma API, permitindo que clientes enviem dados para receber predições do modelo.\n\nA implantação no ACI é ideal para testes e pequenos workloads de inferência, uma vez que oferece escalabilidade sem a necessidade de gerenciamento da infraestrutura."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# Configuração para deployment no ACI\n",
        "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
        "\n",
        "# Realizar a implantação do modelo\n",
        "service = Model.deploy(workspace=ws,\n",
        "                       name=\"anomaly-detector-service\",\n",
        "                       models=[model],  # Modelo que já foi registrado anteriormente\n",
        "                       inference_config=inference_config,\n",
        "                       deployment_config=aci_config)\n",
        "\n",
        "# Aguardar até a conclusão da implantação\n",
        "service.wait_for_deployment(show_output=True)\n",
        "\n",
        "# Exibir a URI de pontuação para enviar requisições\n",
        "print(f\"Service deployed at: {service.scoring_uri}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3353/1094109139.py:8: FutureWarning: azureml.core.model:\nTo leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \nplease refer to respective documentations \nhttps://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\nhttps://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \nFor more information on migration, see https://aka.ms/acimoemigration \nTo disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n  service = Model.deploy(workspace=ws,\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2024-10-14 19:57:48+00:00 Creating Container Registry if not exists.\n2024-10-14 19:57:50+00:00 Building image..\n2024-10-14 20:09:05+00:00 Generating deployment configuration..\n2024-10-14 20:09:06+00:00 Submitting deployment to compute..\n2024-10-14 20:09:16+00:00 Checking the status of deployment anomaly-detector-service..\n2024-10-14 20:10:25+00:00 Checking the status of inference endpoint anomaly-detector-service.\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nService deployed at: http://3e863a0d-d990-4a1b-926e-1d1ffe7a73fc.swedencentral.azurecontainer.io/score\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728936627691
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enviando Dados para o Serviço de Inferência no Azure Machine Learning\n\nEste código faz uma requisição **POST** para um serviço de inferência no **Azure Machine Learning**. Ele envia dados de exemplo ao modelo implantado e retorna as predições do modelo. Abaixo está a explicação de cada parte do código.\n\n## 1. **Importando Bibliotecas Necessárias**\n\n```python\nimport requests\nimport json\n```\n\n- **`requests`**: Biblioteca usada para enviar requisições HTTP, neste caso, uma requisição **POST** para o serviço de inferência no Azure.\n- **`json`**: Biblioteca usada para converter dados de entrada para o formato JSON, que é o formato aceito pelo serviço de inferência.\n\n## 2. **Definindo a URL do Serviço de Inferência**\n\n```python\nscoring_uri = service.scoring_uri\n```\n\n- A variável `scoring_uri` armazena a URL (URI de pontuação) do serviço de inferência implantado no Azure.\n- **Nota**: Se você estiver executando o código localmente, precisaria substituir essa variável pela **URI real** do serviço de inferência, que pode ser algo como `\"https://<nome-do-serviço>.azurewebsites.net/score\"`.\n\n## 3. **Definindo os Dados de Exemplo para Teste**\n\n```python\ninput_data = {\n    \"input\": [\n        [0.5],  # Dados normais (previsão esperada: 1)\n        [10.0],  # Dados fora do padrão (esperado: -1)\n        [0.4]   # Dados normais (previsão esperada: 1)\n    ]\n}\n```\n\n- Os dados de entrada são fornecidos no formato de um dicionário Python. \n  - Cada lista interna (por exemplo, `[0.5]` ou `[10.0]`) representa uma amostra de dados que o modelo utilizará para fazer a previsão.\n  - A previsão esperada seria:\n    - **1** para dados normais (ex: `[0.5]` e `[0.4]`)\n    - **-1** para dados fora do padrão (ex: `[10.0]`).\n\n## 4. **Convertendo os Dados para JSON**\n\n```python\ninput_data_json = json.dumps(input_data)\n```\n\n- A função `json.dumps()` converte o dicionário de dados Python em uma string no formato **JSON**. Isso é necessário porque as APIs de inferência geralmente esperam os dados no formato JSON.\n\n## 5. **Cabeçalhos da Requisição**\n\n```python\nheaders = {\"Content-Type\": \"application/json\"}\n```\n\n- O cabeçalho da requisição indica que os dados estão no formato **JSON** (`Content-Type: application/json`). Isso ajuda o serviço de inferência a interpretar corretamente os dados de entrada.\n\n## 6. **Enviando a Requisição para o Serviço**\n\n```python\nresponse = requests.post(scoring_uri, data=input_data_json, headers=headers)\n```\n\n- A função `requests.post()` envia uma requisição **POST** para o serviço web de inferência no Azure.\n  - **`scoring_uri`**: A URL do serviço de inferência, onde os dados serão enviados.\n  - **`data=input_data_json`**: Os dados de entrada no formato JSON.\n  - **`headers=headers`**: Os cabeçalhos da requisição que especificam que o conteúdo enviado é JSON.\n\n## 7. **Exibindo a Resposta**\n\n```python\nprint(response.json())\n```\n\n- A função `response.json()` converte a resposta da API de volta para um formato Python (geralmente um dicionário), que é impresso no console.\n- A resposta do serviço provavelmente incluirá uma lista de predições para os dados fornecidos. Cada predição será:\n  - **1** para amostras que o modelo considera normais.\n  - **-1** para amostras que o modelo considera anômalas.\n\n## Resumo\n\nEste código realiza uma requisição POST para um serviço de inferência no **Azure Machine Learning**, enviando dados de entrada e recebendo predições do modelo implantado. Os dados de entrada são convertidos para JSON e enviados para a **URI de pontuação** do serviço, e a resposta, que contém as predições do modelo, é exibida no console. Esse fluxo é útil para testar o comportamento do modelo com diferentes entradas e garantir que o serviço esteja funcionando corretamente em produção."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# URL do serviço implantado (substitua pela URI correta)\n",
        "scoring_uri = service.scoring_uri\n",
        "\n",
        "# Dados de exemplo para teste\n",
        "input_data = {\n",
        "    \"input\": [\n",
        "        [0.5],  # Dados normais (previsão esperada: 1)\n",
        "        [10.0],  # Dados fora do padrão (esperado: -1)\n",
        "        [0.4]   # Dados normais (previsão esperada: 1)\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Converter dados para JSON\n",
        "input_data_json = json.dumps(input_data)\n",
        "\n",
        "# Cabeçalhos da requisição\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Enviar a requisição POST para o endpoint\n",
        "response = requests.post(scoring_uri, data=input_data_json, headers=headers)\n",
        "\n",
        "# Exibir a resposta\n",
        "print(response.json())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'predictions': [1, 1, 1], 'alerts': []}\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728939146068
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperando logs de execução\n",
        "\n",
        "A função `service.get_logs` é usada para recuperar os logs gerados pelo serviço de inferência implantado no **Azure Machine Learning**. Esses logs podem ser úteis para depurar o comportamento do serviço, monitorar erros ou verificar o estado da implantação.\n",
        "\n",
        "No entanto, para realmente exibir os logs, você precisa chamar a função `get_logs()` com parênteses, assim:\n",
        "\n",
        "```python\n",
        "print(service.get_logs())\n",
        "```\n",
        "\n",
        "Essa linha de código:\n",
        "\n",
        "- **`service.get_logs()`**: Chama o método que recupera os logs do serviço.\n",
        "- **`print()`**: Exibe os logs no console.\n",
        "\n",
        "## O que esperar dos logs\n",
        "\n",
        "Os logs retornados podem incluir:\n",
        "- Mensagens de inicialização do serviço.\n",
        "- Logs do contêiner que executa o modelo.\n",
        "- Logs de requisições HTTP recebidas, incluindo status de sucesso ou erro.\n",
        "- Erros de inferência, se houver.\n",
        "\n",
        "Esses logs são úteis para verificar se o serviço está funcionando corretamente ou para depurar possíveis problemas no código ou no ambiente de inferência."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(service.get_logs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<bound method Webservice.get_logs of AciWebservice(workspace=Workspace.create(name='ml-workspace-training', subscription_id='32855dba-b7b2-4fdb-8288-f0615afa0cdf', resource_group='rg-evaluations'), name=anomaly-detector-service, image_id=None, image_digest=None, compute_type=ACI, state=Unhealthy, scoring_uri=http://980ecd8c-495f-4619-9f44-dc05a0afcd4a.swedencentral.azurecontainer.io/score, tags={}, properties={'hasInferenceSchema': 'False', 'hasHttps': 'False', 'authEnabled': 'False'}, created_by={'userObjectId': '387398dd-48ba-4eb8-9f5b-9d7b373ea1c0', 'userPuId': '10032002FD96959D', 'userIdp': None, 'userAltSecId': None, 'userIss': 'https://sts.windows.net/6bd72601-6bad-49d9-82df-a390b167cd68/', 'userTenantId': '6bd72601-6bad-49d9-82df-a390b167cd68', 'userName': 'Rodrigo Tavares', 'upn': 'tavares@netplataforma.com.br'})>\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1728934659923
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}